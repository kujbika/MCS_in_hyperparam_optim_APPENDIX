sigma2 <- 1.2
estimator <- function(y, modelling_params) {
row_number = length(y) - modelling_params + 1
vec = matrix(rev(y)[1 : modelling_params + 1], ncol = modelling_params)
for (i in 3:row_number) {
vec = rbind(vec, matrix(rev(y)[i : (i + modelling_params - 1)], ncol=modelling_params))
}
X = cbind(1, vec)
X_Mp_inverse <- solve(t(X) %*% X) %*% t(X)
phi = X_Mp_inverse %*% head(rev(y), row_number-1)
sigma2_est =  (1/row_number) * t(head(rev(y), row_number-1) - X %*% phi) %*% (head(rev(y), row_number-1) - X %*% phi)
return (list(phi, sigma2_est))
}
#-------------------------------------------------------------
resid_generator <- function(y, i_vec) {
# y is the time series that follows an AR(p)
# i_vec is a vector with the possible orders, e.g 1:8 if the real order is p=4
resids = c()
for (k in i_vec) {
phi_k = estimator(y, k)[[1]]
x = c(1,rev(y)[2:(k+1)])
y_resid_k = (x %*% phi_k - rev(y)[1])^2
resids = c(resids, y_resid_k)
}
return (resids) }
grid_search <- function(alpha, phi, sigma2, i_vec) {
y = arima.sim(n = 2000, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y, i_vec)
phi_i = estimator(y, which.min(resids))
prediction = c(1, rev(y)[1:(length(phi_i[[1]])-1)]) %*% phi_i[[1]]
real = c(1, rev(y)[1:length(phi)]) %*% c(alpha, phi) + rnorm(1, 0, sigma2)
return ((prediction - real)^2)
}
loss_values <- function(alpha, phi, sigma2) {
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
for (k in 1:1000)
{
y = arima.sim(n = 3000, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y, 1:8)
loss = rbind(loss, c(resids))
print(k)
}
return (loss[-1,])
}
a = loss_values(alpha, phi, sigma2)
3 %1
mod(3,1)
floor(3/1)
floor(10/10)
a = loss_values(alpha, phi, sigma2)
cor(a)
summary(a)
View(a)
#######mcs
number_cores <- detectCores()
cl <- makeCluster(number_cores)
MCS <- MCSprocedure(Loss=a,alpha=0.1,B=5000,statistic='Tmax',cl=cl)
MCS <- MCSprocedure(Loss=a,alpha=0.05,B=5000,statistic='Tmax',cl=cl)
MCS <- MCSprocedure(Loss=a,alpha=0.01,B=5000,statistic='Tmax',cl=cl)
loss_values <- function(alpha, phi, sigma2) {
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
for (k in 1:1000)
{
y = arima.sim(n = 5000, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y, 1:8)
loss = rbind(loss, c(resids))
print(k)
}
return (loss[-1,])
}
Loss = loss_values(alpha, phi, sigma2)
cor(a)
summary(a)
MCS <- MCSprocedure(Loss=a,alpha=0.1,B=5000,statistic='Tmax',cl=cl)
Loss
y = arima.sim(n = 100, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
y
tail(y)
tail(y, 1)
head(Loss)
arima.fit(y)
arima(y)
u = y[1,length(y)-50]
u = y[1:length(y)-50]
y
u = y[1:(length(y)-50)]
u
prediction <- function(y, k, n.ahead) {
phi_k = estimator(y, k)[[1]]
for (j in 1:n.ahead){
prediction = c(1, rev(y)[1:(length(phi_i)-1)]) %*% phi_i
y=c(y, prediction)
}
return (tail(y, n.ahead))
}
y = arima.sim(n = 10, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
y
phi
y
prediction(y, 2, 3)
prediction <- function(y, k, n.ahead) {
phi_k = estimator(y, k)[[1]]
for (j in 1:n.ahead){
prediction = c(1, rev(y)[1:(length(phi_k)-1)]) %*% phi_k
y=c(y, prediction)
}
return (tail(y, n.ahead))
}
prediction(y, 2, 3)
y
prediction <- function(y, k, n.ahead) {
phi_k = estimator(y, k)[[1]]
for (j in 1:n.ahead){
prediction = c(1, rev(y)[1:(length(phi_k)-1)]) %*% phi_k
y=c(y, prediction)
}
return (y)
}
prediction(y, 2, 3)
y
prediction <- function(y, k, n.ahead) {
phi_k = estimator(y, k)[[1]]
for (j in 1:n.ahead){
prediction = c(1, rev(y)[1:(length(phi_k)-1)]) %*% phi_k
y=c(y, prediction)
}
return (tail(y, n.ahead))
}
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
y = arima.sim(n = 1500, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
u = y[1:(length(y) - n.ahead)]
n.ahead=50
u = y[1:(length(y) - n.ahead)]
tail(y, 60)
tail(u, 10)
i=1
(prediction(u, i, n.ahead) - tail(y, n.ahead))^2
loss[,i]=(prediction(u, i, n.ahead) - tail(y, n.ahead))^2
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss= data.frame(matrix(0, nrow=n.ahead, ncol=9))
names(loss)=c("loss1", "loss2","loss3","loss4","loss5","loss6","loss7","loss8")
loss
y = arima.sim(n = 1500, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
u = y[1:(length(y) - n.ahead)]
for (i in 1:8) {
loss[,i]=(prediction(u, i, n.ahead) - tail(y, n.ahead))^2
}
loss
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss= data.frame(matrix(0, nrow=n.ahead, ncol=8))
names(loss)=c("loss1", "loss2","loss3","loss4","loss5","loss6","loss7","loss8")
y = arima.sim(n = 1500, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
u = y[1:(length(y) - n.ahead)]
for (i in 1:8) {
loss[,i]=(prediction(u, i, n.ahead) - tail(y, n.ahead))^2
}
loss
summaary(loss)
summary(loss)
apply(loss, 1, sum)
apply(loss, 2, sum)
min(apply(loss, 2, sum))
cl = makeCluster(detectCores())
alpha=0.1
mcs = MCSprocedure(loss, alpha, cl=cl)
y = arima.sim(n = 3000, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
n.ahead=20
u = y[1:(length(y) - n.ahead)]
for (i in 1:8) {
loss[,i]=(prediction(u, i, n.ahead) - tail(y, n.ahead))^2
}
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss= data.frame(matrix(0, nrow=n.ahead, ncol=8))
names(loss)=c("loss1", "loss2","loss3","loss4","loss5","loss6","loss7","loss8")
for (i in 1:8) {
loss[,i]=(prediction(u, i, n.ahead) - tail(y, n.ahead))^2
}
mcs = MCSprocedure(loss, alpha, cl=cl)
mcs = MCSprocedure(loss, 0.15, cl=cl)
#-------------------------------------------------------------
resid_generator <- function(y, i_vec) {
# y is the time series that follows an AR(p)
# i_vec is a vector with the possible orders, e.g 1:8 if the real order is p=4
resids = c()
preds=c()
for (k in i_vec) {
phi_k = estimator(y, k)[[1]]
x = c(1,rev(y)[2:(k+1)])
y_resid_k = (x %*% phi_k - rev(y)[1])^2
resids = c(resids, y_resid_k)
p=c(1, rev(y)[1:(length(phi_k)-1)]) %*% phi_k
preds=c(preds, p)
}
return (list(resids, preds)) }
y = arima.sim(n = 1001, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y[1:(length(y)-1)], 1:8)
resids
y
tail(y)
loss_values <- function(alpha, phi, sigma2) {
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
preds = data.frame("pred_1" = NA,
"pred_2" = NA,
"pred_3" = NA,
"pred_4" = NA,
"pred_5" = NA,
"pred_6" = NA,
"pred_7" = NA,
"pred_8" = NA)
for (k in 1:1000)
{
y = arima.sim(n = 1001, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y[1:(length(y)-1)], 1:8)
loss = rbind(loss, c(resids[[1]]))
preds = rbind(preds, c(resids[[2]]))
print(k)
}
return (list(loss[-1,], preds[-1,]))
}
loss_values <- function(alpha, phi, sigma2) {
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
preds = data.frame("pred_1" = NA,
"pred_2" = NA,
"pred_3" = NA,
"pred_4" = NA,
"pred_5" = NA,
"pred_6" = NA,
"pred_7" = NA,
"pred_8" = NA)
for (k in 1:1000)
{
y = arima.sim(n = 1001, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y[1:(length(y)-1)], 1:8)
loss = rbind(loss, c(resids[[1]]))
preds = rbind(preds, c(resids[[2]]))
print(k)
}
return (list(loss[-1,], preds[-1,]))
}
res = loss_values(alpha, phi, sigma2)
resids
resids[[2]]
resids[[2]]-0
resids[[2]]-10
y = arima.sim(n = 1001, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y[1:(length(y)-1)], 1:8)
resids
outsample = (resids[[2]] - tail(y, 1))^2
outsample
alpha
phi
sigma2
alpha <- 5
phi <-  c(0.8, .25, -0.4, 0.1) # phi_1, phi_2, phi_3. order matters!
isStationer <- function(phi) {
if (min(abs(polyroot(c(1, -phi)))) <=1) {
print('Phi does not describe a stationary process')
phi = as.double(strsplit(readline("Define a new phi vector, with the following format: phi1,phi2,..."), ",")[[1]])
return (isStationer(phi))
} else return(phi)
}
phi = isStationer(phi)
sigma2 <- 1.2
alpha
res = loss_values(alpha, phi, sigma2)
Loss=res[[1]]
summary(Loss)
Loss
head(Loss)
plot(Loss$loss_1)
plot(Loss$loss_3)
plot(Loss$loss_5)
plot(Loss$loss_7)
hist(Loss$loss_7)
hist(Loss$loss_1)
hist(Loss$loss_2)
hist(Loss$loss_3)
hist(Loss$loss_2)
Loss[i, ]
mins=c(mins, preds[i, which.min(Loss[i, ])])
preds[i, which.min(Loss[i, ])
]
preds=res[[2]]
preds[i, which.min(Loss[i, ])]
i
head(Loss)
head(Loss, 8)
which.min(Loss[i, ])
Loss[i, which.min(Loss[i,])]
Loss[i, ]
i
Loss[1, ]
preds[i,]
preds[i-1,]
preds[i+1,]
head(preds)
loss_values <- function(alpha, phi, sigma2) {
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
preds = data.frame("pred_1" = NA,
"pred_2" = NA,
"pred_3" = NA,
"pred_4" = NA,
"pred_5" = NA,
"pred_6" = NA,
"pred_7" = NA,
"pred_8" = NA)
for (k in 1:1000)
{
y = arima.sim(n = 1001, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y[1:(length(y)-1)], 1:8)
loss = rbind(loss, c(resids[[1]]))
outsample = (resids[[2]] - tail(y, 1))^2
preds = rbind(preds, c(outsample))
print(k)
}
return (list(loss[-1,], preds[-1,]))
}
res = loss_values(alpha, phi, sigma2)
Loss=res[[1]]
preds=res[[2]]
head(preds)
head(Loss)
plot(preds$pred_1)
hist(preds$pred_1)
hist(preds$pred_5)
hist(preds$pred_6)
hist(preds$pred_4)
mins=c()
for (i in 1:nrow(Loss)) {
mins=c(mins, preds[i, which.min(Loss[i, ])])
}
hist(mins)
mean(mins)
mins=c()
for (i in 1:nrow(Loss)) {
mins=c(mins, Loss[i, which.min(Loss[i, ])])
}
mean(mins)
mins=c()
for (i in 1:nrow(Loss)) {
mins=c(mins, preds[i, which.min(Loss[i, ])])
}
mean(mins)
grid_search_loss=c()
for (i in 1:nrow(Loss)) {
grid_search_loss=c(grid_search_loss, preds[i, which.min(Loss[i, ])])
}
mean(grid_search_loss)
a=MCSprocedure(Loss, alpha=0.1, cl=cl)
a
a
?MCSprocedure
apply(preds,1,sum)
apply(preds,2,sum)
apply(preds,2,sum)/nrow(preds)
b=apply(preds,2,sum)/nrow(preds)
mean(b[3:8])
a=MCSprocedure(Loss, alpha=0.15, cl=cl)
a
loss_values <- function(alpha, phi, sigma2) {
# alpha is the intercept of the AR(p)
# phi is the real coeff. vector of the same AR(p)
# this is only run for experimantla purposes, so i_vec here is alwayzs 1:8
loss = data.frame("loss_1" = NA,
"loss_2" = NA,
"loss_3" = NA,
"loss_4" = NA,
"loss_5" = NA,
"loss_6" = NA,
"loss_7" = NA,
"loss_8" = NA)
preds = data.frame("pred_1" = NA,
"pred_2" = NA,
"pred_3" = NA,
"pred_4" = NA,
"pred_5" = NA,
"pred_6" = NA,
"pred_7" = NA,
"pred_8" = NA)
for (k in 1:1000)
{
y = arima.sim(n = 1001, list(ar=c(phi)), sd=sqrt(sigma2), mean=alpha)
resids = resid_generator(y[1:(length(y)-1)], 1:8)
loss = rbind(loss, c(resids[[1]]))
outsample = (resids[[2]] - tail(y, 1))^2
preds = rbind(preds, c(outsample))
}
Loss=loss[-1,]
preds=preds[-1,]
grid_search_loss=c()
for (i in 1:nrow(Loss)) {
grid_search_loss=c(grid_search_loss, preds[i, which.min(Loss[i, ])])
}
MCS_loss=(apply(preds, 2, sum)/nrow(preds))[4:8]
return (list(grid_search_loss, MCS_loss))
}
alpha
pphi
phi
sigma()
sigma2
preds
Loss
plot(colMeans(Loss), type="l")
plot(colMeans(preds), type="l")
plot(colMeans(Loss), type="l")
plot(colMeans(preds), type="l")
cor(preds)
a = colMeans(preds)
a
str(a)
type(a)
data.frame(a)
data.frame(t(a))
ggplot(data=t(a))+geom_point()
ggplot(data=data.frame(t(a)))+geom_point()
data.frame(a)
ggplot(data=data.frame(a))+geom_point()
ggplot(data=data.frame(a))+geom_point(aes(y=a))
a = data.frame(colMeans(preds))
ggplot(data=a)+geom_point(aes(y=a, x=))
a$colMeans.preds.
a
names(a)=c("model", "los")
names(a)=c("model", "loss")
index(a)
rownames(a)
ggplot(data=a)+geom_point(aes(y=a, x=rownames(a)))
a$model=rownames(a)
names(a)[1]="loss"
a
ggplot(data=a)+geom_point(aes(y=loss, x=model)
)
ggplot(data=a)+geom_line(aes(y=loss, x=model))
ggplot(data=a)+ggline(aes(y=loss, x=model))
ggplot(data=a)+gg_line(aes(y=loss, x=model))
ggplot(data=a)+geom_line(aes(y=loss, x=model))
a
ggplot(data=a)+geom_point(aes(y=loss, x=model))
ggplot(data=a, aes(y=loss, x=model))+geom_point()
ggplot(data=a, aes(y=loss, x=model))+geom_line()
ggplot(data=a, aes(y=loss, x=model, group=1))+geom_line()
Loss
save.image("forplot.RData")
p=ggplot(data=a, aes(y=loss, x=model, group=1))+geom_line()
save.image("forplot.RData")
setwd("C:/Marci/CEU/ThesisDONTUSETHIS/CODE/R/Covid19_Prediction")
load("covidPred_secondoptim.RData")
MCS # p value was 0.15
